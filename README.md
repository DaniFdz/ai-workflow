# ðŸ¦ž MiniDani

**Competitive parallel AI development system**

MiniDani runs 3 AI coding agents in parallel competing to implement your feature, then automatically selects and merges the best solution.

---

## What is MiniDani?

MiniDani creates **competitive pressure** between AI agents to produce better code:

1. **Launch 3 Managers** (A, B, C) in parallel
2. Each manager independently implements your feature using [OpenCode](https://github.com/unit-mesh/opencode)
3. Each works in an isolated git worktree
4. A **Judge** evaluates all 3 implementations (0-100 score)
5. Winner is kept, losers are cleaned up
6. PR description is auto-generated

**Smart retry:** If all scores < 80, MiniDani automatically runs Round 2 with improvement feedback.

---

## Quick Start

### Requirements

```bash
pip install -r requirements.txt
```

**Dependencies:**
- Python 3.8+
- Git
- [OpenCode CLI](https://github.com/unit-mesh/opencode) installed at `~/.opencode/bin/opencode`
- Rich library (for TUI, auto-installed via requirements.txt)

### Usage

```bash
cd /path/to/your/project
python3 /path/to/minidani.py "Add OAuth2 authentication with JWT tokens"
```

**What happens:**
- 3 parallel implementations start immediately
- Live TUI shows progress (phases, managers, scores, activity log)
- After 5-10 minutes, you have the best solution auto-selected
- Winner branch is ready for PR

---

## How It Works

### Architecture

![MiniDani Workflow](assets/workflow-diagram.png)

**6-Phase Process:**

```
User Prompt
    â†“
[Phase 1] Generate branch name â†’ feature/oauth-auth
    â†“
[Phase 2] Create 3 git worktrees (isolated workspaces)
    â†“
[Phase 3] Run 3 Managers in parallel threads:
          Manager A: feature/oauth-auth-r1-a
          Manager B: feature/oauth-auth-r1-b  
          Manager C: feature/oauth-auth-r1-c
    â†“
[Phase 4] Judge evaluates all 3:
          A=87, B=95, C=82 â†’ Winner: B
    â†“
[Phase 5] Cleanup: Delete worktrees A and C
    â†“
[Phase 6] Generate PR description from winner
    â†“
âœ… Ready to merge feature/oauth-auth-r1-b
```

### Retry Logic (Automatic Quality Assurance)

If **all** scores are below 80:
- Round 2 launches automatically
- Managers get feedback about Round 1 failures
- Focus on: complete implementation, tests, docs, error handling
- Best solution from either round wins

Example:
```
Round 1: A=45, B=50, C=40  âš ï¸  Low quality detected
         â†“
Round 2: A=85, B=88, C=82  âœ… Winner: B (88/100)
```

---

## Judging Criteria

Judges evaluate on 4 dimensions:

| Criterion | Weight | What it measures |
|-----------|--------|------------------|
| **Completeness** | 35% | Implements all requirements? |
| **Code Quality** | 30% | Clean, maintainable, readable? |
| **Correctness** | 25% | Works correctly, handles edge cases? |
| **Best Practices** | 10% | Tests, docs, error handling? |

**Scoring guide:**
- **90-100**: Excellent - complete, tested, documented
- **80-89**: Good - functional with minor issues
- **70-79**: Acceptable - works but needs polish
- **<70**: Insufficient - missing features or broken

---

## Live TUI Interface

While running, you see:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ¦ž MiniDani [Round 1]                      â”‚
â”‚ feature/oauth-auth | 00:04:32               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Phases â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€ Managers (Round 1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… 1. Branch  100%â”‚  â”‚ ðŸ¤– Manager A                   â”‚
â”‚ âœ… 2. Setup   100%â”‚  â”‚    ðŸ”„ running (i3)             â”‚
â”‚ ðŸ”„ 3. Managers 67%â”‚  â”‚    Implementing auth logic     â”‚
â”‚ â³ 4. Judge     0%â”‚  â”‚                                â”‚
â”‚ â³ 5. Cleanup   0%â”‚  â”‚ ðŸ¤– Manager B                   â”‚
â”‚ â³ 6. PR        0%â”‚  â”‚    ðŸ”„ running (i4)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    Writing tests               â”‚
                        â”‚    ðŸ† Score: 88/100            â”‚
                        â”‚                                â”‚
                        â”‚ ðŸ¤– Manager C                   â”‚
                        â”‚    âœ… complete (i2)            â”‚
                        â”‚    Done                        â”‚
                        â”‚    Score: 75/100               â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        
â”Œâ”€ Activity Log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 18:22:15 [MA] ðŸ”„ Start R1                    â”‚
â”‚ 18:22:17 [MB] ðŸ”„ Start R1                    â”‚
â”‚ 18:22:19 [MC] ðŸ”„ Start R1                    â”‚
â”‚ 18:25:42 [MC] âœ… OK R1                       â”‚
â”‚ 18:26:35 [MA] âœ… OK R1                       â”‚
â”‚ 18:27:18 [MB] âœ… OK R1                       â”‚
â”‚ 18:27:20 [Judge] âš–ï¸ Scores R1: A=87,B=88,C=75â”‚
â”‚ 18:27:21 [Judge] ðŸ† Winner: B                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 ðŸ† Winner: B
```

---

## Configuration

### Quality Threshold

Edit `minidani.py`:

```python
self.QUALITY_THRESHOLD = 80  # Change to 70 or 90
```

### Timeouts

```python
# Manager execution timeout (default: 8 minutes per manager)
r = self.run_oc(..., timeout=480)

# Judge timeout (default: 2 minutes)
r = self.run_oc(..., timeout=120)
```

---

## Project Structure

```
minidani/
â”œâ”€â”€ minidani.py              # Main script (TUI + retry logic)
â”œâ”€â”€ requirements.txt         # Python dependencies (rich)
â”œâ”€â”€ INSTALL.md              # Installation guide
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ LICENSE                 # MIT license
â”‚
â”œâ”€â”€ .opencode/              # OpenCode agent templates (reference)
â”‚   â”œâ”€â”€ agents/             # Agent definitions (not directly used)
â”‚   â””â”€â”€ skills/             # Skills (for future extensions)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md     # Technical design
â”‚   â””â”€â”€ QUICKSTART.md       # 5-minute guide
â”‚
â””â”€â”€ examples/
    â”œâ”€â”€ simple-task.sh      # Example: simple feature
    â””â”€â”€ complex-task.sh     # Example: complex feature
```

**Note:** The `.opencode/agents/*.md` files are **reference templates** showing how a multi-agent system could be structured. The actual implementation directly calls OpenCode with custom prompts (see `minidani.py`).

---

## Examples

### Simple Task

```bash
python3 minidani.py "Create a function to validate email addresses with regex"
```

**Result:** 3 implementations compete, best one selected in ~2 minutes.

### Complex Task

```bash
python3 minidani.py "Build a REST API with:
- User authentication (JWT)
- CRUD operations for posts
- SQLite database
- Pytest test suite
- Docker setup"
```

**Result:** 3 full implementations compete, best one selected in ~10 minutes.

### With Retry

```bash
python3 minidani.py "Hello world script"
```

**Result:**
```
Round 1: A=45, B=50, C=40  âš ï¸  Too simple, low scores
Round 2: A=85, B=88, C=82  âœ… Improved with feedback
Winner: B (88/100)
```

---

## Troubleshooting

### OpenCode not found

```bash
# Check installation
ls -la ~/.opencode/bin/opencode

# Add to PATH if needed
export PATH="$HOME/.opencode/bin:$PATH"
```

### Worktree conflicts

```bash
# List worktrees
git worktree list

# Remove stuck worktree
git worktree remove /path/to/worktree --force

# Remove branch
git branch -D feature/name-r1-a
```

### All scores below 50

**Causes:**
- Prompt too simple (judge expects more)
- Prompt ambiguous (managers confused)
- OpenCode having issues

**Solution:** MiniDani will auto-retry. If Round 2 also fails, check OpenCode logs.

---

## Advanced Usage

### Custom Repository Path

Edit `minidani.py`:

```python
if __name__ == "__main__":
    # Change from /tmp/minidani-test-repo to your repo
    minidani = MiniDaniRetry(
        Path("/path/to/your/repo"),
        " ".join(sys.argv[1:])
    )
```

### Run Without TUI (headless)

Comment out the `with Live(...)` block and use plain print statements.

### Customize Manager Count

Currently hardcoded to 3 (A, B, C). To change:

1. Edit `self.state.managers = {...}` in `__init__`
2. Update all loops: `for m in ["a","b","c"]` â†’ `for m in ["a","b","c","d"]`
3. Update Phase 2 setup and cleanup logic

---

## How Is This Different?

| Approach | Description | Pros | Cons |
|----------|-------------|------|------|
| **Single AI** | One agent implements | Fast | Limited quality |
| **Iterative refinement** | Agent + feedback loop | Improved quality | Time-consuming |
| **MiniDani** | 3 parallel + judge | Best quality, same time | Uses 3x compute |

**MiniDani's advantage:** Competitive pressure + parallel execution = better results in the same time as a single refined attempt.

---

## Limitations

- **OpenCode dependency:** Requires OpenCode CLI installed and working
- **Compute cost:** Runs 3 instances in parallel (3x API calls)
- **Git worktrees:** Requires clean git state, no conflicts
- **No LangChain/Autogen:** Direct OpenCode subprocess calls (simpler, more reliable)

---

## Future Ideas

- [ ] Support 5+ managers
- [ ] Web UI instead of terminal TUI
- [ ] Support other AI coding tools (Aider, Cursor, Claude Code)
- [ ] Persistent judging database (learn what "good" means)
- [ ] Team mode (Red team implements, Blue team reviews)
- [ ] Streaming logs from OpenCode in real-time

---

## Contributing

Improvements welcome:

1. Fork the repo
2. Create branch: `git checkout -b feature/improvement`
3. Test with multiple tasks
4. Create PR with clear description

---

## License

MIT License - Use freely, improve, share.

---

## Credits

- **[OpenCode](https://github.com/unit-mesh/opencode)** - AI coding tool powering each manager
- **[Rich](https://github.com/Textualize/rich)** - Beautiful TUI library
- **Claude/Anthropic** - AI model behind OpenCode

---

## Contact

- **Issues:** [GitHub Issues](https://github.com/DaniFdz/ai-workflow/issues)
- **Discussions:** [GitHub Discussions](https://github.com/DaniFdz/ai-workflow/discussions)

---

**Version:** 1.0.0  
**Last updated:** 2026-01-31  
**Author:** DaniFdz (with help from JuanBot ðŸ¦ž)
